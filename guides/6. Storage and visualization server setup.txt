####Work in progress####


This guide assumes that operational syslog server on Ubuntu Server 14.04 LTS virtual machine with working network connection is installed and accessible via SSH. Basic understanding of vim command is assumed but configuration can be carried out with other editors as well. Functional custom CA setup is required with make_cert.sh script. Syslog server must be operational. Commands must be carried out as root user, execution via sudo is possible but untested. Relevant entries must be appended to presented configuration files or, if already present, altered according to temlates within this guide. Relevant services must be restarted for changes to take effect.

Storage + visualization serveri is configured separately from regular correlation server (Guide 2.). This is due to the fact that described components require java (personal preference, but I do not like mixing it with other solututions). Server can be used independantly as central log collector, or , in my case, all logs from syslog-ng correlation server will be forwarded here for storage and visualization.

Please change domain.ex with your own domain name. Same goes for server names.

1. Set up certificates

# Create certificates 
# Ref. Guide 3. (Server side points 1. and 2.)


2. Forward all logs from relay syslog-ng to storage server

# In case you plan to configure this on separate box
# I did this in testing, but live is configured directly on top of existing server (the one that was configured in Guide 3.)
# On relay server

vim /etc/syslog-ng/syslog-ng.conf

"""
...
destination d_kibana_server {
        syslog(
                kibana.domain.ex
                transport("tls")
                port(6514)
                tls( peer-verify(required-trusted) ca_dir('/etc/ssl/syslog/') key_file('/etc/ssl/syslog/orion.domain.ex-key.pem') cert_file('/etc/ssl/syslog/orion.domain.ex-cert.pem'))
        );
};
...
log {
        source(s_remote_ietf);
        source(s_src);
        destination(d_local);
        destination(d_kibana_server);
};
...
"""

service syslog-ng restart

3. Set up elasticsearch server

apt-get install openjdk-7-jre-headless -y

# Variant 1 - manual download (skip to Variant 2)
cd /opt/
export elastic_version=elasticsearch-1.2.0.deb
wget https://download.elasticsearch.org/elasticsearch/elasticsearch/$elastic_version
dpkg -i $elastic_version

# Variant 2 - repo

wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
echo 'deb http://packages.elasticsearch.org/elasticsearch/1.1/debian stable main' | tee /etc/apt/sources.list.d/elasticsearch.list
apt-get update
apt-get install elasticsearch=1.1.1

# add to startup
update-rc.d elasticsearch defaults 95 10

# start server
/etc/init.d/elasticsearch start

# https://www.digitalocean.com/community/tutorials/how-to-use-logstash-and-kibana-to-centralize-and-visualize-logs-on-ubuntu-14-04

4. Configure log forwarder (eighter Rsyslog or Logstash, not both)

4.1. Rsyslog (IGNORE THIS)

# NOTE - This is informative, my solution is configured using a logstash file input with grok filters
# We use rsyslog 8 ppa, since earlier version does not provide elasticsearch plugin
# You can compile by hand but I consider this approach cruel and unusual punishment for any sysadmin

add-apt-repository ppa:adiscon/v8-stable
apt-get update && apt-get dist-upgrade && apt-get dist-upgrade
apt-get -y install rsyslog-elasticsearch rsyslog-gnutls

vim /etc/rsyslog.conf

"""
...
$ModLoad imtcp
...
"""

vim /etc/rsyslog.d/remote.conf

"""
# TLS driver load
$DefaultNetstreamDriver gtls

# certificate files
$DefaultNetstreamDriverCAFile /etc/ssl/syslog/cacert.pem
$DefaultNetstreamDriverCertFile /etc/ssl/syslog/cert.pem
$DefaultNetstreamDriverKeyFile /etc/ssl/syslog/key.pem

$ActionSendStreamDriverAuthMode x509/name
$InputTCPServerStreamDriverPermittedPeer *.domain.ex
$InputTCPServerStreamDriverMode 1 # run driver in TLS-only mode
$InputTCPServerRun 6514 # start up listener at port 6514
"""

# For simple change in domain name value, 
# Don't forget to escape the dot (matching is done using regular expressions; unescaped dot means "match anything")

export domain_name=<your_domain_name>

sed -i s/domain\.ex/$domain_name/g /etc/rsyslog.d/remote.conf

service rsyslog restart

vim /etc/rsyslog.d/elasticsearch.conf

#from http://www.rsyslog.com/guides-for-rsyslog/the-recipies/more-complex-scenarios/

"""
module(load="imuxsock")             # for listening to /dev/log
module(load="omelasticsearch") # for outputting to Elasticsearch
# this is for index names to be like: logstash-YYYY.MM.DD
template(name="logstash-index"
  type="list") {
    constant(value="logstash-")
    property(name="timereported" dateFormat="rfc3339" position.from="1" position.to="4")
    constant(value=".")
    property(name="timereported" dateFormat="rfc3339" position.from="6" position.to="7")
    constant(value=".")
    property(name="timereported" dateFormat="rfc3339" position.from="9" position.to="10")
}

# this is for formatting our syslog in JSON with @timestamp
template(name="plain-syslog"
  type="list") {
    constant(value="{")
      constant(value="\"@timestamp\":\"")     property(name="timereported" dateFormat="rfc3339")
      constant(value="\",\"host\":\"")        property(name="hostname")
      constant(value="\",\"severity\":\"")    property(name="syslogseverity-text")
      constant(value="\",\"facility\":\"")    property(name="syslogfacility-text")
      constant(value="\",\"tag\":\"")   property(name="syslogtag" format="json")
      constant(value="\",\"message\":\"")    property(name="msg" format="json")
    constant(value="\"}")
}
# this is where we actually send the logs to Elasticsearch (localhost:9200 by default)
action(type="omelasticsearch"
    template="plain-syslog"
    searchIndex="logstash-index"
    dynSearchIndex="on")
"""

/etc/init.d/rsyslog restart

4.2. Logstash (USE THIS)

#Install the packages
echo 'deb http://packages.elasticsearch.org/logstash/1.4/debian stable main' |  tee /etc/apt/sources.list.d/logstash.list
apt-get update
apt-get install logstash

#Configure logstash
#Firstly, create a pattern for apache vhost_combined access logs

/etc/logstash/patterns/apache.pattern

"""
VHOSTCOMMONAPACHELOG %{IPORHOST:http_vhost}:?%{IPORHOST:http_port}? %{IPORHOST:http_clientip} %{USER:http_ident} %{USER:http_auth} \[%{HTTPDATE:http_timestamp}\] "(?:%{WORD:http_method} %{NOTSPACE:http_request}(?: HTTP/%{NUMBER:http_version})?|%{DATA:http_rawrequest})" %{NUMBER:http_response} (?:%{NUMBER:http_bytes}|-) "%{DATA:http_referer}" "%{DATA:http_useragent}"
"""

#Then configure a file parser for access logs
#all relevant data is parsed using a grok filter (combined with the previous definition)
#we also use geoip module to extract data per client IP

vim /etc/logstash/conf.d/10-syslog.conf

"""
#input { tcp { host => "127.0.0.1" port => 5000 type => syslog } }

input {
        file {
                path => "/var/log/server/daemon/apache.notice.log" # array (required)
                type => "apacheaccess" # string (optional)
        }
}

filter {
#  if [type] == "syslog" {
#    grok {
#      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
#      add_field => [ "received_at", "%{@timestamp}" ]
#      add_field => [ "received_from", "%{host}" ]
#    }
#    syslog_pri {
#      type => "syslog"
#      severity_labels => ["emergency", "alert", "critical", "error", "warning", "notice", "informational", "debug"]
#    }
#    date {
#      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
#    }
#  }
        if [type] == "apacheaccess" {
                grok {
                        patterns_dir => "/etc/logstash/patterns/"
                        match => [ "message", "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{VHOSTCOMMONAPACHELOG}" ]
                }
                geoip {
                        add_tag => [ "GeoIP" ]
                        source => "http_clientip"
                }
        }
        date {
                match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        }
}

output {
        elasticsearch { host => "localhost" }
        stdout { codec => rubydebug }
}
"""

5. Set up kibana3

5.1. Set up apache vhost template 

# Install apache web server

apt-get install apache2
a2dissite 000-default
service apache2 reload

# We use HTTPS by default
vim /etc/apache2/sites-available/vhost_ssl_template

"""
<VirtualHost *:80>
        ServerName __VHOST__
        RewriteEngine On
        RewriteCond %{HTTPS} off
        RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI}
</VirtualHost>

<VirtualHost *:443>
        ServerName __VHOST__

        SSLEngine on
        SSLCertificateFile /etc/apache2/__VHOST__/__VHOST__.crt
        SSLCertificateKeyFile /etc/apache2/id_rsa.key

        DocumentRoot /srv/virtuals/__VHOST__/html/
        <Directory /srv/virtuals/__VHOST__/html/>
                Require all granted
                Options -Multiviews
        </Directory>

        LogLevel debug
        ErrorLog /srv/virtuals/__VHOST__/log/error.log
        CustomLog /srv/virtuals/__VHOST__/access.log vhost_combined

        # Set global proxy timeouts
        <Proxy http://127.0.0.1:9200>
                ProxySet connectiontimeout=5 timeout=90
        </Proxy>

        # Proxy for _aliases and .*/_search
        <LocationMatch "^/(_nodes|_aliases|.*/_aliases|_search|.*/_search|_mapping|.*/_mapping)$">
                ProxyPassMatch http://127.0.0.1:9200/$1
                ProxyPassReverse http://127.0.0.1:9200/$1
        </LocationMatch>

        # Proxy for kibana-int/{dashboard,temp} stuff (if you don't want auth on /, then you will want these to be protected)
        <LocationMatch "^/(kibana-int/dashboard/|kibana-int/temp)(.*)$">
                ProxyPassMatch http://127.0.0.1:9200/$1$2
                ProxyPassReverse http://127.0.0.1:9200/$1$2
        </LocationMatch>

        <location />
                AuthType basic
                AuthName "private"
                AuthUserFile /var/secure/.htpasswd
                Require valid-user
        </location>

#       # Optional disable auth for a src IP (eg: your monitoring host or subnet)
#       # Apache 2.2 syntax
#       <Location />
#               Allow from 5.6.7.8
#               Deny from all
#               Satisfy any
#
#               AuthLDAPBindDN "CN=_ldapbinduser,OU=Users,DC=example,DC=com"
#               AuthLDAPBindPassword "ldapbindpass"
#               AuthLDAPURL "ldaps://ldap01.example.com ldap02.example.com/OU=Users,DC=example,DC=com?sAMAccountName?sub?(objectClass=*)"
#               AuthType Basic
#               AuthBasicProvider ldap
#               AuthName "Please authenticate for Example dot com"
#               AuthLDAPGroupAttributeIsDN on
#               require valid-user
#       </Location>

</VirtualHost>
"""

5.2. Basic authenticaiton

apt-get install apache2-utils

mkdir /var/secure

chown www-data /var/secure/ && chmod 700 /var/secure/

htpasswd -c /var/secure/.htpasswd admin

/etc/init.d/apache2 reload

5.3. Create apache vhost for kibana

export VHOST=kibana.domain.ex
mkdir -p /srv/virtuals/$VHOST/{html,log}

ssh-keygen -t rsa -b 2048

#save in /etc/apache2/id_rsa.key

mkdir /etc/apache2/$VHOST && openssl req -new -key /etc/apache2/id_rsa.key -out /etc/apache2/$VHOST/$VHOST.csr

#CN=$VHOST

openssl x509 -req -days 1825 -in /etc/apache2/$VHOST/$VHOST.csr -signkey /etc/apache2/id_rsa.key -out /etc/apache2/$VHOST/$VHOST.crt

cp /etc/apache2/sites-available/vhost_ssl_template /etc/apache2/sites-available/$VHOST-ssl.conf

sed -i s/__VHOST__/$VHOST/g /etc/apache2/sites-available/$VHOST-ssl.conf

a2enmod ssl rewrite proxy proxy_http
a2ensite $VHOST-ssl.conf

/etc/init.d/apache2 reload

5.4. kibana

apt-get install unzip
cd /opt

wget http://download.elasticsearch.org/kibana/kibana/kibana-latest.zip

unzip kibana-latest.zip

mv kibana-latest/* /srv/virtuals/$VHOST/html/ && cd /srv/virtuals/$VHOST/html/

# configure elasticsearch backend

vim config.js

"""
...
elasticsearch: "https://"+window.location.hostname+":443",
...
"""

6. Iptables

# Remenber that elasticsearch has no built in authentication nor encryption
# That's why we configured a reverse proxy
# Now close off all elasticsearch ports for outside world (don't worry, we use localhost and proxy to keep it functional)

vim /etc/iptables.conf

"""
# Generated by iptables-save v1.4.12 on Fri Mar  7 10:28:40 2014
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [772:63557]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -p tcp -m tcp --dport 22 -m state --state NEW -j ACCEPT
-A INPUT -p tcp -m tcp --dport 80 -m state --state NEW -j ACCEPT
-A INPUT -p tcp -m tcp --dport 443 -m state --state NEW -j ACCEPT
-A INPUT -p tcp -m tcp --dport 6514 -m state --state NEW -j ACCEPT
-A INPUT -j LOG --log-prefix "iptables: " --log-level 5
-A INPUT -j DROP
COMMIT
# Completed on Fri Mar  7 10:28:40 2014
"""

#Add iptables rules to network startup (under primary network interface)

vim /etc/network/interfaces

"""
...
pre-up iptables-restore < /etc/iptables.conf
...
"""